{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cold Email Generator with Llama 3.3 70b, Langchain, ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANIKET\\miniconda3\\envs\\my_langchain_env\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import langchain_groq to get fast API inferences from Llama model\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we create an LLM object with the API key generated through Groq\n",
    "We are using the Llama 3.3 70b model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "# Load environment variables from .env file\n",
    "dotenv.load_dotenv()\n",
    "# Set the environment variable for Groq API key\n",
    "KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n",
    "llm = ChatGroq(\n",
    "    temperature = 0,    \n",
    "    api_key = KEY,\n",
    "    model = \"llama-3.3-70b-versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the LLM by invoking a sample prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"What is your name?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing ChromaDB which is going to be our Vector Database as it is reliable and lightweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a VectorDB object and creating a new collection in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"new_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding sample documents into our collection in the Vector DB\n",
    "We can also notice that Chroma DB is internally using MiniLM-L6-v2 to get the vector embedding from our documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=[\n",
    "        \"This is a sample document about ML\",\n",
    "        \"This is a sample document about Love\"\n",
    "    ],\n",
    "    ids = ['id1', 'id2'],\n",
    "    metadatas=[\n",
    "        {\"url\": \"https://en.wikipedia.org/wiki/Machine_learning\"},\n",
    "        {\"url\": \"https://en.wikipedia.org/wiki/Love\"}\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see how the Vector DB has stored our documents in the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['id1', 'id2'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['This is a sample document about ML',\n",
       "  'This is a sample document about Love'],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents'],\n",
       " 'data': None,\n",
       " 'metadatas': [{'url': 'https://en.wikipedia.org/wiki/Machine_learning'},\n",
       "  {'url': 'https://en.wikipedia.org/wiki/Love'}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs = collection.get()\n",
    "all_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since it's a vector DB, we can query a sample text to get the closest n vectors to it and their respective distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id2', 'id1']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['This is a sample document about Love',\n",
       "   'This is a sample document about ML']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'url': 'https://en.wikipedia.org/wiki/Love'},\n",
       "   {'url': 'https://en.wikipedia.org/wiki/Machine_learning'}]],\n",
       " 'distances': [[1.3829848766326904, 1.902585744857788]]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=['I am in a relationship'],\n",
    "    n_results=2\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id1', 'id2']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['This is a sample document about ML',\n",
       "   'This is a sample document about Love']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'url': 'https://en.wikipedia.org/wiki/Machine_learning'},\n",
       "   {'url': 'https://en.wikipedia.org/wiki/Love'}]],\n",
       " 'distances': [[1.6828935146331787, 1.9099119901657104]]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=['I also like data science'],\n",
    "    n_results=2\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " !pip install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using WebBaseLoader from Langchain to scrape the webpages of given job opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = WebBaseLoader('https://www.google.com/about/careers/applications/jobs/results/126881918376387270-cloud-engineer-ai?q=%22Data%20Scientist%22&location=Bengaluru%2C%20India&target_level=EARLY')\n",
    "loader = WebBaseLoader(\"https://www.amazon.jobs/en/jobs/2960212/applied-scientist-alexa-sensitive-content-intelligence-asci?cmpid=SPLICX0248M&utm_source=linkedin.com&utm_campaign=cxro&utm_medium=social_media&utm_content=job_posting&ss=paid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_data = loader.load().pop().page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied Scientist, Alexa Sensitive Content Intelligence (ASCI) - Job ID: 2960212 | Amazon.jobs\n",
      "Skip to main contentHomeTeamsLocationsJob categoriesMy careerMy applicationsMy profileAccount securitySettingsSign outResourcesDisability accommodationsBenefitsInclusive experiencesInterview tipsLeadership principlesWorking at AmazonFAQ×Applied Scientist, Alexa Sensitive Content Intelligence (ASCI)Job ID: 2960212 | ADCI - BLR 14 SEZApply nowDESCRIPTIONAlexa is the voice activated digital assistant powering devices like Amazon Echo, Echo Dot, Echo Show, and Fire TV, which are at the forefront of this latest technology wave. To preserve our customers’ experience and trust, the Alexa Privacy team creates policies and builds services and tools through Machine Learning techniques to detect and mitigate sensitive content across Alexa. We are looking for an experienced Senior Applied Scientist to build industry-leading technologies in attribute extraction and sensitive content detection across all languages and countries.An Applied Scientist  will be in a team of exceptional scientists to develop novel algorithms and modeling techniques to advance the state of the art in Natural Language Processing (NLP) or Computer Vision (CV) related tasks. They will work in a hybrid, fast-paced organization where scientists, engineers, and product managers work together to build customer facing experiences. They will collaborate with and mentor other scientists to raise the bar of scientific research in Amazon. Their work will directly impact our customers in the form of products and services that make use of speech, language, and computer vision technologies.We are looking for candidate with strong technical experiences and a passion for building scientific driven solutions in a fast-paced environment. This Senior Applied Scientist should have good understanding of NLP models (e.g. LSTM, transformer based models) or CV models (e.g. CNN, AlexNet, ResNet) and where to apply them in different business cases. They should leverage exceptional technical expertise, a sound understanding of the fundamentals of Computer Science, and practical experience of building large-scale distributed systems to creating reliable, scalable, and high-performance products. In addition to technical depth, they must possess exceptional communication skills and understand how to influence key stakeholders.This Applied Scientist will be joining a select group of people making history producing one of the most highly rated products in Amazon's history, so if you are looking for a challenging and innovative role where you can solve important problems while growing as a leader, this may be the place for you.Key job responsibilitiesThis Applied Scientist will lead the science solution design, run experiments, research new algorithms, and find new ways of optimizing customer experience. They will set examples for the team on good science practice and standards. Besides theoretical analysis and innovation, they will work closely with talented engineers and ML scientists to put algorithms and models into practice. This Applied Scientist's work will also directly impact the trust customers place in Alexa, globally. They will contribute directly to our growth by hiring smart and motivated scientists to establish teams that can deliver swiftly and predictably, adjusting in an agile fashion to deliver what our customers need.A day in the lifeYou will be working with a group of talented scientists on researching algorithm and running experiments to test scientific proposal/solutions to improve our sensitive contents detection and mitigation. This will involve collaboration with partner teams including engineering, PMs, data annotators, and other scientists to discuss data quality, policy, and model development. You will mentor other scientists, review and guide their work, help develop roadmaps for the team. You work closely with partner teams across Alexa to deliver platform features that require cross-team leadership.About the teamThe mission of the Alexa Sensitive Content Intelligence (ASCI) team is to (1) minimize negative surprises to customers caused by sensitive content, (2) detect and prevent potential brand-damaging interactions, and (3) build customer trust through appropriate interactions on sensitive topics.The term “sensitive content” includes within its scope a wide range of categories of content such as offensive content (e.g., hate speech, racist speech), profanity, content that is suitable only for certain age groups, politically polarizing content, and religiously polarizing content. The term “content” refers to any material that is exposed to customers by Alexa (including both 1P and 3P experiences) and includes text, speech, audio, and video.BASIC QUALIFICATIONS- 3+ years of building models for business application experience- PhD, or Master's degree and 4+ years of CS, CE, ML or related field experience- Experience in patents or publications at top-tier peer-reviewed conferences or journals- Experience programming in Java, C++, Python or related language- Experience in any of the following areas: algorithms and data structures, parsing, numerical optimization, data mining, parallel and distributed computing, high-performance computingPREFERRED QUALIFICATIONS- Experience using Unix/Linux- Experience in professional software developmentOur inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you’re applying in isn’t listed, please contact your Recruiting Partner.Job detailsIND, KA, BengaluruAlexa and Echo DevicesMachine Learning ScienceShare this jobJOIN US ONFind CareersJob CategoriesTeamsLocationsUS and EU Military recruitingWarehouse and Hourly JobsWorking At AmazonCultureBenefitsAmazon NewsletterInclusive experiencesOur leadership principlesHelpFAQInterview tipsReview application statusDisability accommodationsLegal disclosures and noticesAmazon is an equal opportunity employer and does not discriminate on the basis of protected veteran status, disability, or other legally protected status.Privacy and DataImpressum© 1996-2025, Amazon.com, Inc. or its affiliates\n"
     ]
    }
   ],
   "source": [
    "print(page_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a prompt template using langchain prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_extract = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "            ### SCRAPED TEXT FROM WEBSITE:\n",
    "            {data}\n",
    "            ### INSTRUCTION:\n",
    "            The scraped text is from the career's page of a website.\n",
    "            Your task is to extract the job postings and return them in JSON format containing the \n",
    "            following keys: role, experience, skills and description.\n",
    "            ### VALID JSON, NO PREAMBLE:\n",
    "        \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a pipeline to create the prompt from the scraped data and then giving that prompt to our LLM to get the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_extract = prompt_extract | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"role\": \"Applied Scientist, Alexa Sensitive Content Intelligence (ASCI)\",\n",
      "  \"experience\": \"3+ years of building models for business application experience\",\n",
      "  \"skills\": [\n",
      "    \"NLP models (e.g. LSTM, transformer based models)\",\n",
      "    \"CV models (e.g. CNN, AlexNet, ResNet)\",\n",
      "    \"Java, C++, Python or related language\",\n",
      "    \"algorithms and data structures, parsing, numerical optimization, data mining, parallel and distributed computing, high-performance computing\"\n",
      "  ],\n",
      "  \"description\": \"We are looking for an experienced Senior Applied Scientist to build industry-leading technologies in attribute extraction and sensitive content detection across all languages and countries.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Invoking the pipeline made in previous step\n",
    "response = chain_extract.invoke(input={'data': page_data})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the output from string format to JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'Applied Scientist, Alexa Sensitive Content Intelligence (ASCI)',\n",
       " 'experience': '3+ years of building models for business application experience',\n",
       " 'skills': ['NLP models (e.g. LSTM, transformer based models)',\n",
       "  'CV models (e.g. CNN, AlexNet, ResNet)',\n",
       "  'Java, C++, Python or related language',\n",
       "  'algorithms and data structures, parsing, numerical optimization, data mining, parallel and distributed computing, high-performance computing'],\n",
       " 'description': 'We are looking for an experienced Senior Applied Scientist to build industry-leading technologies in attribute extraction and sensitive content detection across all languages and countries.'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "json_parser = JsonOutputParser()\n",
    "json_res = json_parser.parse(response.content)\n",
    "json_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Techstack</th>\n",
       "      <th>Links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>React, Node.js, MongoDB</td>\n",
       "      <td>https://example.com/react-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Angular,.NET, SQL Server</td>\n",
       "      <td>https://example.com/angular-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vue.js, Ruby on Rails, PostgreSQL</td>\n",
       "      <td>https://example.com/vue-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Python, Django, MySQL</td>\n",
       "      <td>https://example.com/python-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Java, Spring Boot, Oracle</td>\n",
       "      <td>https://example.com/java-portfolio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Techstack                                  Links\n",
       "0            React, Node.js, MongoDB    https://example.com/react-portfolio\n",
       "1           Angular,.NET, SQL Server  https://example.com/angular-portfolio\n",
       "2  Vue.js, Ruby on Rails, PostgreSQL      https://example.com/vue-portfolio\n",
       "3              Python, Django, MySQL   https://example.com/python-portfolio\n",
       "4          Java, Spring Boot, Oracle     https://example.com/java-portfolio"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('my_portfolio.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(\"Vector_Store\")\n",
    "portfolio = client.create_collection(\"portfolios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not portfolio.count():\n",
    "    for idx, row in df.iterrows():\n",
    "        portfolio.add(\n",
    "            documents=row[\"Techstack\"],\n",
    "            metadatas={\"url\": row[\"Links\"]},\n",
    "            ids=[str(uuid.uuid4())]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['a923b080-3151-4211-b6f6-1b85c1c39361',\n",
       "  '928183b7-7a16-4829-9fcf-bbb26af4cc4c',\n",
       "  '197677f5-7a3d-4a84-ac4b-cb597e07c49c',\n",
       "  '632741e0-aaf1-4ea9-83fd-36bed0675aed',\n",
       "  'fbf50f9f-d981-4cbb-8fc7-679080347d9a',\n",
       "  '48889f17-f2c7-4bc2-8402-43c1189a1b99',\n",
       "  '7bc07b0d-8603-43cb-9842-133884bb405b',\n",
       "  'e8f94b0c-ac0d-4eaa-aedb-71ec7dabe897',\n",
       "  '6e8cc08f-9c06-41f1-892a-a9886c8ea7f2',\n",
       "  '37c62e67-1209-4221-86f0-2cc85746cb2f',\n",
       "  'd350ccdb-17c1-44c6-8dd0-9b9217ef6511',\n",
       "  'c7abc42b-1968-4698-b1d5-31df8db241a2',\n",
       "  '1a105a61-9c86-4431-929d-4953b8744bf2',\n",
       "  '5d9cc418-7b66-40a8-bb6e-69165397cea0',\n",
       "  '9c7f5944-678b-4a0d-8c19-7dbbdbf64df2',\n",
       "  '19ad6a18-a24f-40e8-aaae-1a8a79d3bc21',\n",
       "  'cb64d319-e1d3-465b-995a-4f1365088725',\n",
       "  '71e097ad-11fd-4ef9-9fd4-0f75b79e2ca4',\n",
       "  '54c92071-c13e-4f65-aea0-ea6f80c167eb',\n",
       "  '16bdb87c-6603-492a-a993-55f46d912c5f'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['React, Node.js, MongoDB',\n",
       "  'Angular,.NET, SQL Server',\n",
       "  'Vue.js, Ruby on Rails, PostgreSQL',\n",
       "  'Python, Django, MySQL',\n",
       "  'Java, Spring Boot, Oracle',\n",
       "  'Flutter, Firebase, GraphQL',\n",
       "  'WordPress, PHP, MySQL',\n",
       "  'Magento, PHP, MySQL',\n",
       "  'React Native, Node.js, MongoDB',\n",
       "  'iOS, Swift, Core Data',\n",
       "  'Android, Java, Room Persistence',\n",
       "  'Kotlin, Android, Firebase',\n",
       "  'Android TV, Kotlin, Android NDK',\n",
       "  'iOS, Swift, ARKit',\n",
       "  'Cross-platform, Xamarin, Azure',\n",
       "  'Backend, Kotlin, Spring Boot',\n",
       "  'Frontend, TypeScript, Angular',\n",
       "  'Full-stack, JavaScript, Express.js',\n",
       "  'Machine Learning, Python, TensorFlow',\n",
       "  'DevOps, Jenkins, Docker'],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents'],\n",
       " 'data': None,\n",
       " 'metadatas': [{'url': 'https://example.com/react-portfolio'},\n",
       "  {'url': 'https://example.com/angular-portfolio'},\n",
       "  {'url': 'https://example.com/vue-portfolio'},\n",
       "  {'url': 'https://example.com/python-portfolio'},\n",
       "  {'url': 'https://example.com/java-portfolio'},\n",
       "  {'url': 'https://example.com/flutter-portfolio'},\n",
       "  {'url': 'https://example.com/wordpress-portfolio'},\n",
       "  {'url': 'https://example.com/magento-portfolio'},\n",
       "  {'url': 'https://example.com/react-native-portfolio'},\n",
       "  {'url': 'https://example.com/ios-portfolio'},\n",
       "  {'url': 'https://example.com/android-portfolio'},\n",
       "  {'url': 'https://example.com/kotlin-android-portfolio'},\n",
       "  {'url': 'https://example.com/android-tv-portfolio'},\n",
       "  {'url': 'https://example.com/ios-ar-portfolio'},\n",
       "  {'url': 'https://example.com/xamarin-portfolio'},\n",
       "  {'url': 'https://example.com/kotlin-backend-portfolio'},\n",
       "  {'url': 'https://example.com/typescript-frontend-portfolio'},\n",
       "  {'url': 'https://example.com/full-stack-js-portfolio'},\n",
       "  {'url': 'https://example.com/ml-python-portfolio'},\n",
       "  {'url': 'https://example.com/devops-portfolio'}]}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['54c92071-c13e-4f65-aea0-ea6f80c167eb',\n",
       "   '197677f5-7a3d-4a84-ac4b-cb597e07c49c']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Machine Learning, Python, TensorFlow',\n",
       "   'Vue.js, Ruby on Rails, PostgreSQL']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'url': 'https://example.com/ml-python-portfolio'},\n",
       "   {'url': 'https://example.com/vue-portfolio'}]],\n",
       " 'distances': [[1.361631989479065, 1.7005481719970703]]}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_output = portfolio.query(query_texts=response.content, n_results=2)\n",
    "query_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'url': 'https://example.com/ml-python-portfolio'},\n",
       "  {'url': 'https://example.com/vue-portfolio'}]]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = query_output['metadatas']\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_email = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "                ### JOB DESCRIPTION:\n",
    "                {job_description}\n",
    "                \n",
    "                ### INSTRUCTION:\n",
    "                You are Mohan, a business development executive at AtliQ. AtliQ is an AI & Software Consulting company dedicated to facilitating\n",
    "                the seamless integration of business processes through automated tools. \n",
    "                Over our experience, we have empowered numerous enterprises with tailored solutions, fostering scalability, \n",
    "                process optimization, cost reduction, and heightened overall efficiency. \n",
    "                Your job is to write a cold email to the client regarding the job mentioned above describing the capability of AtliQ \n",
    "                in fulfilling their needs.\n",
    "                Also add the most relevant ones from the following links to showcase Atliq's portfolio: {link_list}\n",
    "                Remember you are Mohan, BDE at AtliQ. \n",
    "                Do not provide a preamble.\n",
    "                ### EMAIL (NO PREAMBLE):\n",
    "            \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_email | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chain.invoke(input={'job_description': response.content, 'link_list': links})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Expert Solutions for Alexa Sensitive Content Intelligence (ASCI)\n",
      "\n",
      "Dear Hiring Manager,\n",
      "\n",
      "I came across the job description for an Applied Scientist, Alexa Sensitive Content Intelligence (ASCI), and I am excited to introduce AtliQ, an AI & Software Consulting company that can help you build industry-leading technologies in attribute extraction and sensitive content detection.\n",
      "\n",
      "With over 3+ years of experience in building models for business applications, our team of experts possesses the required skills to fulfill your needs. We have hands-on experience with NLP models (e.g., LSTM, transformer-based models) and CV models (e.g., CNN, AlexNet, ResNet). Our proficiency in programming languages such as Java, C++, Python, and related languages, along with expertise in algorithms and data structures, parsing, numerical optimization, data mining, parallel and distributed computing, and high-performance computing, makes us a perfect fit for this role.\n",
      "\n",
      "At AtliQ, we have empowered numerous enterprises with tailored solutions, fostering scalability, process optimization, cost reduction, and heightened overall efficiency. Our portfolio includes a range of projects that demonstrate our capabilities in machine learning and software development. For instance, you can check out our machine learning projects with Python at https://example.com/ml-python-portfolio.\n",
      "\n",
      "We believe that our expertise and experience make us an ideal partner to help you achieve your goals in building ASCI technologies. I would be delighted to schedule a call to discuss how AtliQ can support your project and provide a customized solution that meets your requirements.\n",
      "\n",
      "Please let me know if you are interested, and I will be happy to set up a call at your convenience.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "Mohan\n",
      "Business Development Executive\n",
      "AtliQ\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10",
   "language": "python",
   "name": "my_langchain_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
